{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv_loader import tweets, test_tweets\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def check_base_word(base_word):\n",
    "\tif base_word and len(base_word) > 1 and base_word.isalpha():\n",
    "\t\tstemmed_word = stemmer.stem(base_word)\n",
    "\t\tif stemmed_word not in base_vector and stemmed_word not in stopwords.words('english'):\n",
    "\t\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "\n",
    "# Create base vector\n",
    "base_vector = []\n",
    "for tweet in tweets:\n",
    "\tif tweet['text']:\n",
    "\t\twords = tweet['text'].split()\n",
    "\t\tfor word in words:\n",
    "\t\t\tif check_base_word(word):\n",
    "\t\t\t\tbase_vector.append(stemmer.stem(word))\n",
    "                \n",
    "\n",
    "# Transform tweets into vectors\n",
    "tweet_vectors = []\n",
    "category_labels = []\n",
    "subcategory_labels = []\n",
    "for tweet in tweets:\n",
    "    category_labels.append(tweet['category'])\n",
    "    subcategory_labels.append(tweet['subcategory'])\n",
    "    words = list(set(tweet['text'].split()))\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word and len(word) > 1 and word.isalpha()]\n",
    "    tweet_vector = []\n",
    "    for ele in base_vector:\n",
    "        tweet_vector.append(1 if ele in stemmed_words else 0)\n",
    "    tweet_vectors.append(tweet_vector)\n",
    "        \n",
    "# Transform tweets into vectors\n",
    "test_tweet_vectors = []\n",
    "test_category_labels = []\n",
    "test_subcategory_labels = []\n",
    "for tweet in test_tweets:\n",
    "    test_category_labels.append(tweet['category'])\n",
    "    test_subcategory_labels.append(tweet['subcategory'])\n",
    "    words = list(set(tweet['text'].split()))\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word and len(word) > 1 and word.isalpha()]\n",
    "    test_tweet_vector = []\n",
    "    for ele in base_vector:\n",
    "        test_tweet_vector.append(1 if ele in stemmed_words else 0)\n",
    "    test_tweet_vectors.append(test_tweet_vector)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tweet_vectors = torch.LongTensor(tweet_vectors)\n",
    "tensor_category_labels = torch.LongTensor(category_labels)\n",
    "tensor_subcategory_labels = torch.LongTensor(subcategory_labels)\n",
    "\n",
    "\n",
    "tensor_test_tweet_vectors = torch.LongTensor(test_tweet_vectors)\n",
    "tensor_test_category_labels = torch.LongTensor(test_category_labels)\n",
    "tensor_test_subcategory_labels = torch.LongTensor(test_subcategory_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1000], Step: [100/3], Loss: 1.5782\n",
      "Epoch: [1/1000], Step: [200/3], Loss: 1.5378\n",
      "Epoch: [1/1000], Step: [300/3], Loss: 1.4995\n",
      "Epoch: [2/1000], Step: [100/3], Loss: 1.4290\n",
      "Epoch: [2/1000], Step: [200/3], Loss: 1.3961\n",
      "Epoch: [2/1000], Step: [300/3], Loss: 1.3649\n",
      "Epoch: [3/1000], Step: [100/3], Loss: 1.3077\n",
      "Epoch: [3/1000], Step: [200/3], Loss: 1.2814\n",
      "Epoch: [3/1000], Step: [300/3], Loss: 1.2566\n",
      "Epoch: [4/1000], Step: [100/3], Loss: 1.2122\n",
      "Epoch: [4/1000], Step: [200/3], Loss: 1.1922\n",
      "Epoch: [4/1000], Step: [300/3], Loss: 1.1737\n",
      "Epoch: [5/1000], Step: [100/3], Loss: 1.1413\n",
      "Epoch: [5/1000], Step: [200/3], Loss: 1.1271\n",
      "Epoch: [5/1000], Step: [300/3], Loss: 1.1141\n",
      "Epoch: [6/1000], Step: [100/3], Loss: 1.0918\n",
      "Epoch: [6/1000], Step: [200/3], Loss: 1.0821\n",
      "Epoch: [6/1000], Step: [300/3], Loss: 1.0733\n",
      "Epoch: [7/1000], Step: [100/3], Loss: 1.0581\n",
      "Epoch: [7/1000], Step: [200/3], Loss: 1.0514\n",
      "Epoch: [7/1000], Step: [300/3], Loss: 1.0453\n",
      "Epoch: [8/1000], Step: [100/3], Loss: 1.0346\n",
      "Epoch: [8/1000], Step: [200/3], Loss: 1.0297\n",
      "Epoch: [8/1000], Step: [300/3], Loss: 1.0252\n",
      "Epoch: [9/1000], Step: [100/3], Loss: 1.0170\n",
      "Epoch: [9/1000], Step: [200/3], Loss: 1.0132\n",
      "Epoch: [9/1000], Step: [300/3], Loss: 1.0095\n",
      "Epoch: [10/1000], Step: [100/3], Loss: 1.0027\n",
      "Epoch: [10/1000], Step: [200/3], Loss: 0.9994\n",
      "Epoch: [10/1000], Step: [300/3], Loss: 0.9962\n",
      "Epoch: [11/1000], Step: [100/3], Loss: 0.9900\n",
      "Epoch: [11/1000], Step: [200/3], Loss: 0.9870\n",
      "Epoch: [11/1000], Step: [300/3], Loss: 0.9841\n",
      "Epoch: [12/1000], Step: [100/3], Loss: 0.9783\n",
      "Epoch: [12/1000], Step: [200/3], Loss: 0.9754\n",
      "Epoch: [12/1000], Step: [300/3], Loss: 0.9726\n",
      "Epoch: [13/1000], Step: [100/3], Loss: 0.9670\n",
      "Epoch: [13/1000], Step: [200/3], Loss: 0.9642\n",
      "Epoch: [13/1000], Step: [300/3], Loss: 0.9614\n",
      "Epoch: [14/1000], Step: [100/3], Loss: 0.9558\n",
      "Epoch: [14/1000], Step: [200/3], Loss: 0.9530\n",
      "Epoch: [14/1000], Step: [300/3], Loss: 0.9502\n",
      "Epoch: [15/1000], Step: [100/3], Loss: 0.9446\n",
      "Epoch: [15/1000], Step: [200/3], Loss: 0.9418\n",
      "Epoch: [15/1000], Step: [300/3], Loss: 0.9390\n",
      "Epoch: [16/1000], Step: [100/3], Loss: 0.9334\n",
      "Epoch: [16/1000], Step: [200/3], Loss: 0.9305\n",
      "Epoch: [16/1000], Step: [300/3], Loss: 0.9276\n",
      "Epoch: [17/1000], Step: [100/3], Loss: 0.9219\n",
      "Epoch: [17/1000], Step: [200/3], Loss: 0.9190\n",
      "Epoch: [17/1000], Step: [300/3], Loss: 0.9160\n",
      "Epoch: [18/1000], Step: [100/3], Loss: 0.9101\n",
      "Epoch: [18/1000], Step: [200/3], Loss: 0.9072\n",
      "Epoch: [18/1000], Step: [300/3], Loss: 0.9042\n",
      "Epoch: [19/1000], Step: [100/3], Loss: 0.8981\n",
      "Epoch: [19/1000], Step: [200/3], Loss: 0.8950\n",
      "Epoch: [19/1000], Step: [300/3], Loss: 0.8919\n",
      "Epoch: [20/1000], Step: [100/3], Loss: 0.8857\n",
      "Epoch: [20/1000], Step: [200/3], Loss: 0.8826\n",
      "Epoch: [20/1000], Step: [300/3], Loss: 0.8794\n",
      "Epoch: [21/1000], Step: [100/3], Loss: 0.8730\n",
      "Epoch: [21/1000], Step: [200/3], Loss: 0.8697\n",
      "Epoch: [21/1000], Step: [300/3], Loss: 0.8665\n",
      "Epoch: [22/1000], Step: [100/3], Loss: 0.8599\n",
      "Epoch: [22/1000], Step: [200/3], Loss: 0.8565\n",
      "Epoch: [22/1000], Step: [300/3], Loss: 0.8532\n",
      "Epoch: [23/1000], Step: [100/3], Loss: 0.8464\n",
      "Epoch: [23/1000], Step: [200/3], Loss: 0.8429\n",
      "Epoch: [23/1000], Step: [300/3], Loss: 0.8395\n",
      "Epoch: [24/1000], Step: [100/3], Loss: 0.8325\n",
      "Epoch: [24/1000], Step: [200/3], Loss: 0.8290\n",
      "Epoch: [24/1000], Step: [300/3], Loss: 0.8254\n",
      "Epoch: [25/1000], Step: [100/3], Loss: 0.8183\n",
      "Epoch: [25/1000], Step: [200/3], Loss: 0.8146\n",
      "Epoch: [25/1000], Step: [300/3], Loss: 0.8110\n",
      "Epoch: [26/1000], Step: [100/3], Loss: 0.8037\n",
      "Epoch: [26/1000], Step: [200/3], Loss: 0.8000\n",
      "Epoch: [26/1000], Step: [300/3], Loss: 0.7962\n",
      "Epoch: [27/1000], Step: [100/3], Loss: 0.7888\n",
      "Epoch: [27/1000], Step: [200/3], Loss: 0.7850\n",
      "Epoch: [27/1000], Step: [300/3], Loss: 0.7812\n",
      "Epoch: [28/1000], Step: [100/3], Loss: 0.7736\n",
      "Epoch: [28/1000], Step: [200/3], Loss: 0.7697\n",
      "Epoch: [28/1000], Step: [300/3], Loss: 0.7659\n",
      "Epoch: [29/1000], Step: [100/3], Loss: 0.7582\n",
      "Epoch: [29/1000], Step: [200/3], Loss: 0.7543\n",
      "Epoch: [29/1000], Step: [300/3], Loss: 0.7504\n",
      "Epoch: [30/1000], Step: [100/3], Loss: 0.7426\n",
      "Epoch: [30/1000], Step: [200/3], Loss: 0.7386\n",
      "Epoch: [30/1000], Step: [300/3], Loss: 0.7347\n",
      "Epoch: [31/1000], Step: [100/3], Loss: 0.7268\n",
      "Epoch: [31/1000], Step: [200/3], Loss: 0.7229\n",
      "Epoch: [31/1000], Step: [300/3], Loss: 0.7189\n",
      "Epoch: [32/1000], Step: [100/3], Loss: 0.7110\n",
      "Epoch: [32/1000], Step: [200/3], Loss: 0.7070\n",
      "Epoch: [32/1000], Step: [300/3], Loss: 0.7031\n",
      "Epoch: [33/1000], Step: [100/3], Loss: 0.6951\n",
      "Epoch: [33/1000], Step: [200/3], Loss: 0.6912\n",
      "Epoch: [33/1000], Step: [300/3], Loss: 0.6872\n",
      "Epoch: [34/1000], Step: [100/3], Loss: 0.6793\n",
      "Epoch: [34/1000], Step: [200/3], Loss: 0.6753\n",
      "Epoch: [34/1000], Step: [300/3], Loss: 0.6714\n",
      "Epoch: [35/1000], Step: [100/3], Loss: 0.6635\n",
      "Epoch: [35/1000], Step: [200/3], Loss: 0.6595\n",
      "Epoch: [35/1000], Step: [300/3], Loss: 0.6556\n",
      "Epoch: [36/1000], Step: [100/3], Loss: 0.6478\n",
      "Epoch: [36/1000], Step: [200/3], Loss: 0.6439\n",
      "Epoch: [36/1000], Step: [300/3], Loss: 0.6400\n",
      "Epoch: [37/1000], Step: [100/3], Loss: 0.6322\n",
      "Epoch: [37/1000], Step: [200/3], Loss: 0.6283\n",
      "Epoch: [37/1000], Step: [300/3], Loss: 0.6245\n",
      "Epoch: [38/1000], Step: [100/3], Loss: 0.6168\n",
      "Epoch: [38/1000], Step: [200/3], Loss: 0.6130\n",
      "Epoch: [38/1000], Step: [300/3], Loss: 0.6092\n",
      "Epoch: [39/1000], Step: [100/3], Loss: 0.6016\n",
      "Epoch: [39/1000], Step: [200/3], Loss: 0.5979\n",
      "Epoch: [39/1000], Step: [300/3], Loss: 0.5941\n",
      "Epoch: [40/1000], Step: [100/3], Loss: 0.5867\n",
      "Epoch: [40/1000], Step: [200/3], Loss: 0.5830\n",
      "Epoch: [40/1000], Step: [300/3], Loss: 0.5793\n",
      "Epoch: [41/1000], Step: [100/3], Loss: 0.5720\n",
      "Epoch: [41/1000], Step: [200/3], Loss: 0.5684\n",
      "Epoch: [41/1000], Step: [300/3], Loss: 0.5647\n",
      "Epoch: [42/1000], Step: [100/3], Loss: 0.5576\n",
      "Epoch: [42/1000], Step: [200/3], Loss: 0.5540\n",
      "Epoch: [42/1000], Step: [300/3], Loss: 0.5505\n",
      "Epoch: [43/1000], Step: [100/3], Loss: 0.5434\n",
      "Epoch: [43/1000], Step: [200/3], Loss: 0.5400\n",
      "Epoch: [43/1000], Step: [300/3], Loss: 0.5365\n",
      "Epoch: [44/1000], Step: [100/3], Loss: 0.5296\n",
      "Epoch: [44/1000], Step: [200/3], Loss: 0.5262\n",
      "Epoch: [44/1000], Step: [300/3], Loss: 0.5228\n",
      "Epoch: [45/1000], Step: [100/3], Loss: 0.5161\n",
      "Epoch: [45/1000], Step: [200/3], Loss: 0.5128\n",
      "Epoch: [45/1000], Step: [300/3], Loss: 0.5095\n",
      "Epoch: [46/1000], Step: [100/3], Loss: 0.5029\n",
      "Epoch: [46/1000], Step: [200/3], Loss: 0.4997\n",
      "Epoch: [46/1000], Step: [300/3], Loss: 0.4964\n",
      "Epoch: [47/1000], Step: [100/3], Loss: 0.4900\n",
      "Epoch: [47/1000], Step: [200/3], Loss: 0.4869\n",
      "Epoch: [47/1000], Step: [300/3], Loss: 0.4837\n",
      "Epoch: [48/1000], Step: [100/3], Loss: 0.4775\n",
      "Epoch: [48/1000], Step: [200/3], Loss: 0.4744\n",
      "Epoch: [48/1000], Step: [300/3], Loss: 0.4713\n",
      "Epoch: [49/1000], Step: [100/3], Loss: 0.4652\n",
      "Epoch: [49/1000], Step: [200/3], Loss: 0.4622\n",
      "Epoch: [49/1000], Step: [300/3], Loss: 0.4592\n",
      "Epoch: [50/1000], Step: [100/3], Loss: 0.4533\n",
      "Epoch: [50/1000], Step: [200/3], Loss: 0.4504\n",
      "Epoch: [50/1000], Step: [300/3], Loss: 0.4475\n",
      "Epoch: [51/1000], Step: [100/3], Loss: 0.4417\n",
      "Epoch: [51/1000], Step: [200/3], Loss: 0.4389\n",
      "Epoch: [51/1000], Step: [300/3], Loss: 0.4360\n",
      "Epoch: [52/1000], Step: [100/3], Loss: 0.4304\n",
      "Epoch: [52/1000], Step: [200/3], Loss: 0.4276\n",
      "Epoch: [52/1000], Step: [300/3], Loss: 0.4249\n",
      "Epoch: [53/1000], Step: [100/3], Loss: 0.4194\n",
      "Epoch: [53/1000], Step: [200/3], Loss: 0.4167\n",
      "Epoch: [53/1000], Step: [300/3], Loss: 0.4140\n",
      "Epoch: [54/1000], Step: [100/3], Loss: 0.4087\n",
      "Epoch: [54/1000], Step: [200/3], Loss: 0.4061\n",
      "Epoch: [54/1000], Step: [300/3], Loss: 0.4035\n",
      "Epoch: [55/1000], Step: [100/3], Loss: 0.3983\n",
      "Epoch: [55/1000], Step: [200/3], Loss: 0.3958\n",
      "Epoch: [55/1000], Step: [300/3], Loss: 0.3932\n",
      "Epoch: [56/1000], Step: [100/3], Loss: 0.3882\n",
      "Epoch: [56/1000], Step: [200/3], Loss: 0.3857\n",
      "Epoch: [56/1000], Step: [300/3], Loss: 0.3832\n",
      "Epoch: [57/1000], Step: [100/3], Loss: 0.3784\n",
      "Epoch: [57/1000], Step: [200/3], Loss: 0.3760\n",
      "Epoch: [57/1000], Step: [300/3], Loss: 0.3735\n",
      "Epoch: [58/1000], Step: [100/3], Loss: 0.3688\n",
      "Epoch: [58/1000], Step: [200/3], Loss: 0.3665\n",
      "Epoch: [58/1000], Step: [300/3], Loss: 0.3641\n",
      "Epoch: [59/1000], Step: [100/3], Loss: 0.3595\n",
      "Epoch: [59/1000], Step: [200/3], Loss: 0.3572\n",
      "Epoch: [59/1000], Step: [300/3], Loss: 0.3550\n",
      "Epoch: [60/1000], Step: [100/3], Loss: 0.3505\n",
      "Epoch: [60/1000], Step: [200/3], Loss: 0.3483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [60/1000], Step: [300/3], Loss: 0.3461\n",
      "Epoch: [61/1000], Step: [100/3], Loss: 0.3417\n",
      "Epoch: [61/1000], Step: [200/3], Loss: 0.3395\n",
      "Epoch: [61/1000], Step: [300/3], Loss: 0.3374\n",
      "Epoch: [62/1000], Step: [100/3], Loss: 0.3332\n",
      "Epoch: [62/1000], Step: [200/3], Loss: 0.3311\n",
      "Epoch: [62/1000], Step: [300/3], Loss: 0.3290\n",
      "Epoch: [63/1000], Step: [100/3], Loss: 0.3249\n",
      "Epoch: [63/1000], Step: [200/3], Loss: 0.3229\n",
      "Epoch: [63/1000], Step: [300/3], Loss: 0.3208\n",
      "Epoch: [64/1000], Step: [100/3], Loss: 0.3169\n",
      "Epoch: [64/1000], Step: [200/3], Loss: 0.3149\n",
      "Epoch: [64/1000], Step: [300/3], Loss: 0.3129\n",
      "Epoch: [65/1000], Step: [100/3], Loss: 0.3090\n",
      "Epoch: [65/1000], Step: [200/3], Loss: 0.3071\n",
      "Epoch: [65/1000], Step: [300/3], Loss: 0.3052\n",
      "Epoch: [66/1000], Step: [100/3], Loss: 0.3015\n",
      "Epoch: [66/1000], Step: [200/3], Loss: 0.2996\n",
      "Epoch: [66/1000], Step: [300/3], Loss: 0.2977\n",
      "Epoch: [67/1000], Step: [100/3], Loss: 0.2941\n",
      "Epoch: [67/1000], Step: [200/3], Loss: 0.2923\n",
      "Epoch: [67/1000], Step: [300/3], Loss: 0.2905\n",
      "Epoch: [68/1000], Step: [100/3], Loss: 0.2870\n",
      "Epoch: [68/1000], Step: [200/3], Loss: 0.2852\n",
      "Epoch: [68/1000], Step: [300/3], Loss: 0.2835\n",
      "Epoch: [69/1000], Step: [100/3], Loss: 0.2800\n",
      "Epoch: [69/1000], Step: [200/3], Loss: 0.2783\n",
      "Epoch: [69/1000], Step: [300/3], Loss: 0.2766\n",
      "Epoch: [70/1000], Step: [100/3], Loss: 0.2733\n",
      "Epoch: [70/1000], Step: [200/3], Loss: 0.2716\n",
      "Epoch: [70/1000], Step: [300/3], Loss: 0.2700\n",
      "Epoch: [71/1000], Step: [100/3], Loss: 0.2668\n",
      "Epoch: [71/1000], Step: [200/3], Loss: 0.2652\n",
      "Epoch: [71/1000], Step: [300/3], Loss: 0.2636\n",
      "Epoch: [72/1000], Step: [100/3], Loss: 0.2604\n",
      "Epoch: [72/1000], Step: [200/3], Loss: 0.2589\n",
      "Epoch: [72/1000], Step: [300/3], Loss: 0.2573\n",
      "Epoch: [73/1000], Step: [100/3], Loss: 0.2543\n",
      "Epoch: [73/1000], Step: [200/3], Loss: 0.2528\n",
      "Epoch: [73/1000], Step: [300/3], Loss: 0.2513\n",
      "Epoch: [74/1000], Step: [100/3], Loss: 0.2483\n",
      "Epoch: [74/1000], Step: [200/3], Loss: 0.2469\n",
      "Epoch: [74/1000], Step: [300/3], Loss: 0.2454\n",
      "Epoch: [75/1000], Step: [100/3], Loss: 0.2425\n",
      "Epoch: [75/1000], Step: [200/3], Loss: 0.2411\n",
      "Epoch: [75/1000], Step: [300/3], Loss: 0.2397\n",
      "Epoch: [76/1000], Step: [100/3], Loss: 0.2369\n",
      "Epoch: [76/1000], Step: [200/3], Loss: 0.2356\n",
      "Epoch: [76/1000], Step: [300/3], Loss: 0.2342\n",
      "Epoch: [77/1000], Step: [100/3], Loss: 0.2315\n",
      "Epoch: [77/1000], Step: [200/3], Loss: 0.2302\n",
      "Epoch: [77/1000], Step: [300/3], Loss: 0.2288\n",
      "Epoch: [78/1000], Step: [100/3], Loss: 0.2262\n",
      "Epoch: [78/1000], Step: [200/3], Loss: 0.2249\n",
      "Epoch: [78/1000], Step: [300/3], Loss: 0.2236\n",
      "Epoch: [79/1000], Step: [100/3], Loss: 0.2211\n",
      "Epoch: [79/1000], Step: [200/3], Loss: 0.2198\n",
      "Epoch: [79/1000], Step: [300/3], Loss: 0.2186\n",
      "Epoch: [80/1000], Step: [100/3], Loss: 0.2161\n",
      "Epoch: [80/1000], Step: [200/3], Loss: 0.2149\n",
      "Epoch: [80/1000], Step: [300/3], Loss: 0.2137\n",
      "Epoch: [81/1000], Step: [100/3], Loss: 0.2113\n",
      "Epoch: [81/1000], Step: [200/3], Loss: 0.2101\n",
      "Epoch: [81/1000], Step: [300/3], Loss: 0.2089\n",
      "Epoch: [82/1000], Step: [100/3], Loss: 0.2066\n",
      "Epoch: [82/1000], Step: [200/3], Loss: 0.2055\n",
      "Epoch: [82/1000], Step: [300/3], Loss: 0.2043\n",
      "Epoch: [83/1000], Step: [100/3], Loss: 0.2021\n",
      "Epoch: [83/1000], Step: [200/3], Loss: 0.2009\n",
      "Epoch: [83/1000], Step: [300/3], Loss: 0.1998\n",
      "Epoch: [84/1000], Step: [100/3], Loss: 0.1977\n",
      "Epoch: [84/1000], Step: [200/3], Loss: 0.1966\n",
      "Epoch: [84/1000], Step: [300/3], Loss: 0.1955\n",
      "Epoch: [85/1000], Step: [100/3], Loss: 0.1934\n",
      "Epoch: [85/1000], Step: [200/3], Loss: 0.1923\n",
      "Epoch: [85/1000], Step: [300/3], Loss: 0.1913\n",
      "Epoch: [86/1000], Step: [100/3], Loss: 0.1892\n",
      "Epoch: [86/1000], Step: [200/3], Loss: 0.1882\n",
      "Epoch: [86/1000], Step: [300/3], Loss: 0.1872\n",
      "Epoch: [87/1000], Step: [100/3], Loss: 0.1852\n",
      "Epoch: [87/1000], Step: [200/3], Loss: 0.1842\n",
      "Epoch: [87/1000], Step: [300/3], Loss: 0.1832\n",
      "Epoch: [88/1000], Step: [100/3], Loss: 0.1813\n",
      "Epoch: [88/1000], Step: [200/3], Loss: 0.1803\n",
      "Epoch: [88/1000], Step: [300/3], Loss: 0.1793\n",
      "Epoch: [89/1000], Step: [100/3], Loss: 0.1775\n",
      "Epoch: [89/1000], Step: [200/3], Loss: 0.1765\n",
      "Epoch: [89/1000], Step: [300/3], Loss: 0.1756\n",
      "Epoch: [90/1000], Step: [100/3], Loss: 0.1738\n",
      "Epoch: [90/1000], Step: [200/3], Loss: 0.1729\n",
      "Epoch: [90/1000], Step: [300/3], Loss: 0.1720\n",
      "Epoch: [91/1000], Step: [100/3], Loss: 0.1702\n",
      "Epoch: [91/1000], Step: [200/3], Loss: 0.1693\n",
      "Epoch: [91/1000], Step: [300/3], Loss: 0.1684\n",
      "Epoch: [92/1000], Step: [100/3], Loss: 0.1667\n",
      "Epoch: [92/1000], Step: [200/3], Loss: 0.1658\n",
      "Epoch: [92/1000], Step: [300/3], Loss: 0.1650\n",
      "Epoch: [93/1000], Step: [100/3], Loss: 0.1633\n",
      "Epoch: [93/1000], Step: [200/3], Loss: 0.1625\n",
      "Epoch: [93/1000], Step: [300/3], Loss: 0.1616\n",
      "Epoch: [94/1000], Step: [100/3], Loss: 0.1600\n",
      "Epoch: [94/1000], Step: [200/3], Loss: 0.1592\n",
      "Epoch: [94/1000], Step: [300/3], Loss: 0.1584\n",
      "Epoch: [95/1000], Step: [100/3], Loss: 0.1568\n",
      "Epoch: [95/1000], Step: [200/3], Loss: 0.1560\n",
      "Epoch: [95/1000], Step: [300/3], Loss: 0.1552\n",
      "Epoch: [96/1000], Step: [100/3], Loss: 0.1537\n",
      "Epoch: [96/1000], Step: [200/3], Loss: 0.1529\n",
      "Epoch: [96/1000], Step: [300/3], Loss: 0.1521\n",
      "Epoch: [97/1000], Step: [100/3], Loss: 0.1506\n",
      "Epoch: [97/1000], Step: [200/3], Loss: 0.1499\n",
      "Epoch: [97/1000], Step: [300/3], Loss: 0.1491\n",
      "Epoch: [98/1000], Step: [100/3], Loss: 0.1477\n",
      "Epoch: [98/1000], Step: [200/3], Loss: 0.1470\n",
      "Epoch: [98/1000], Step: [300/3], Loss: 0.1462\n",
      "Epoch: [99/1000], Step: [100/3], Loss: 0.1448\n",
      "Epoch: [99/1000], Step: [200/3], Loss: 0.1441\n",
      "Epoch: [99/1000], Step: [300/3], Loss: 0.1434\n",
      "Epoch: [100/1000], Step: [100/3], Loss: 0.1420\n",
      "Epoch: [100/1000], Step: [200/3], Loss: 0.1413\n",
      "Epoch: [100/1000], Step: [300/3], Loss: 0.1407\n",
      "Epoch: [101/1000], Step: [100/3], Loss: 0.1393\n",
      "Epoch: [101/1000], Step: [200/3], Loss: 0.1386\n",
      "Epoch: [101/1000], Step: [300/3], Loss: 0.1380\n",
      "Epoch: [102/1000], Step: [100/3], Loss: 0.1367\n",
      "Epoch: [102/1000], Step: [200/3], Loss: 0.1360\n",
      "Epoch: [102/1000], Step: [300/3], Loss: 0.1354\n",
      "Epoch: [103/1000], Step: [100/3], Loss: 0.1341\n",
      "Epoch: [103/1000], Step: [200/3], Loss: 0.1335\n",
      "Epoch: [103/1000], Step: [300/3], Loss: 0.1328\n",
      "Epoch: [104/1000], Step: [100/3], Loss: 0.1316\n",
      "Epoch: [104/1000], Step: [200/3], Loss: 0.1310\n",
      "Epoch: [104/1000], Step: [300/3], Loss: 0.1304\n",
      "Epoch: [105/1000], Step: [100/3], Loss: 0.1291\n",
      "Epoch: [105/1000], Step: [200/3], Loss: 0.1285\n",
      "Epoch: [105/1000], Step: [300/3], Loss: 0.1280\n",
      "Epoch: [106/1000], Step: [100/3], Loss: 0.1268\n",
      "Epoch: [106/1000], Step: [200/3], Loss: 0.1262\n",
      "Epoch: [106/1000], Step: [300/3], Loss: 0.1256\n",
      "Epoch: [107/1000], Step: [100/3], Loss: 0.1245\n",
      "Epoch: [107/1000], Step: [200/3], Loss: 0.1239\n",
      "Epoch: [107/1000], Step: [300/3], Loss: 0.1233\n",
      "Epoch: [108/1000], Step: [100/3], Loss: 0.1222\n",
      "Epoch: [108/1000], Step: [200/3], Loss: 0.1217\n",
      "Epoch: [108/1000], Step: [300/3], Loss: 0.1211\n",
      "Epoch: [109/1000], Step: [100/3], Loss: 0.1200\n",
      "Epoch: [109/1000], Step: [200/3], Loss: 0.1195\n",
      "Epoch: [109/1000], Step: [300/3], Loss: 0.1189\n",
      "Epoch: [110/1000], Step: [100/3], Loss: 0.1179\n",
      "Epoch: [110/1000], Step: [200/3], Loss: 0.1173\n",
      "Epoch: [110/1000], Step: [300/3], Loss: 0.1168\n",
      "Epoch: [111/1000], Step: [100/3], Loss: 0.1158\n",
      "Epoch: [111/1000], Step: [200/3], Loss: 0.1153\n",
      "Epoch: [111/1000], Step: [300/3], Loss: 0.1148\n",
      "Epoch: [112/1000], Step: [100/3], Loss: 0.1138\n",
      "Epoch: [112/1000], Step: [200/3], Loss: 0.1133\n",
      "Epoch: [112/1000], Step: [300/3], Loss: 0.1128\n",
      "Epoch: [113/1000], Step: [100/3], Loss: 0.1118\n",
      "Epoch: [113/1000], Step: [200/3], Loss: 0.1113\n",
      "Epoch: [113/1000], Step: [300/3], Loss: 0.1108\n",
      "Epoch: [114/1000], Step: [100/3], Loss: 0.1099\n",
      "Epoch: [114/1000], Step: [200/3], Loss: 0.1094\n",
      "Epoch: [114/1000], Step: [300/3], Loss: 0.1089\n",
      "Epoch: [115/1000], Step: [100/3], Loss: 0.1080\n",
      "Epoch: [115/1000], Step: [200/3], Loss: 0.1075\n",
      "Epoch: [115/1000], Step: [300/3], Loss: 0.1070\n",
      "Epoch: [116/1000], Step: [100/3], Loss: 0.1061\n",
      "Epoch: [116/1000], Step: [200/3], Loss: 0.1057\n",
      "Epoch: [116/1000], Step: [300/3], Loss: 0.1052\n",
      "Epoch: [117/1000], Step: [100/3], Loss: 0.1043\n",
      "Epoch: [117/1000], Step: [200/3], Loss: 0.1039\n",
      "Epoch: [117/1000], Step: [300/3], Loss: 0.1035\n",
      "Epoch: [118/1000], Step: [100/3], Loss: 0.1026\n",
      "Epoch: [118/1000], Step: [200/3], Loss: 0.1022\n",
      "Epoch: [118/1000], Step: [300/3], Loss: 0.1017\n",
      "Epoch: [119/1000], Step: [100/3], Loss: 0.1009\n",
      "Epoch: [119/1000], Step: [200/3], Loss: 0.1005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [119/1000], Step: [300/3], Loss: 0.1001\n",
      "Epoch: [120/1000], Step: [100/3], Loss: 0.0992\n",
      "Epoch: [120/1000], Step: [200/3], Loss: 0.0988\n",
      "Epoch: [120/1000], Step: [300/3], Loss: 0.0984\n",
      "Epoch: [121/1000], Step: [100/3], Loss: 0.0976\n",
      "Epoch: [121/1000], Step: [200/3], Loss: 0.0972\n",
      "Epoch: [121/1000], Step: [300/3], Loss: 0.0968\n",
      "Epoch: [122/1000], Step: [100/3], Loss: 0.0960\n",
      "Epoch: [122/1000], Step: [200/3], Loss: 0.0956\n",
      "Epoch: [122/1000], Step: [300/3], Loss: 0.0953\n",
      "Epoch: [123/1000], Step: [100/3], Loss: 0.0945\n",
      "Epoch: [123/1000], Step: [200/3], Loss: 0.0941\n",
      "Epoch: [123/1000], Step: [300/3], Loss: 0.0937\n",
      "Epoch: [124/1000], Step: [100/3], Loss: 0.0930\n",
      "Epoch: [124/1000], Step: [200/3], Loss: 0.0926\n",
      "Epoch: [124/1000], Step: [300/3], Loss: 0.0922\n",
      "Epoch: [125/1000], Step: [100/3], Loss: 0.0915\n",
      "Epoch: [125/1000], Step: [200/3], Loss: 0.0912\n",
      "Epoch: [125/1000], Step: [300/3], Loss: 0.0908\n",
      "Epoch: [126/1000], Step: [100/3], Loss: 0.0901\n",
      "Epoch: [126/1000], Step: [200/3], Loss: 0.0897\n",
      "Epoch: [126/1000], Step: [300/3], Loss: 0.0894\n",
      "Epoch: [127/1000], Step: [100/3], Loss: 0.0887\n",
      "Epoch: [127/1000], Step: [200/3], Loss: 0.0883\n",
      "Epoch: [127/1000], Step: [300/3], Loss: 0.0880\n",
      "Epoch: [128/1000], Step: [100/3], Loss: 0.0873\n",
      "Epoch: [128/1000], Step: [200/3], Loss: 0.0870\n",
      "Epoch: [128/1000], Step: [300/3], Loss: 0.0866\n",
      "Epoch: [129/1000], Step: [100/3], Loss: 0.0860\n",
      "Epoch: [129/1000], Step: [200/3], Loss: 0.0856\n",
      "Epoch: [129/1000], Step: [300/3], Loss: 0.0853\n",
      "Epoch: [130/1000], Step: [100/3], Loss: 0.0847\n",
      "Epoch: [130/1000], Step: [200/3], Loss: 0.0843\n",
      "Epoch: [130/1000], Step: [300/3], Loss: 0.0840\n",
      "Epoch: [131/1000], Step: [100/3], Loss: 0.0834\n",
      "Epoch: [131/1000], Step: [200/3], Loss: 0.0831\n",
      "Epoch: [131/1000], Step: [300/3], Loss: 0.0828\n",
      "Epoch: [132/1000], Step: [100/3], Loss: 0.0821\n",
      "Epoch: [132/1000], Step: [200/3], Loss: 0.0818\n",
      "Epoch: [132/1000], Step: [300/3], Loss: 0.0815\n",
      "Epoch: [133/1000], Step: [100/3], Loss: 0.0809\n",
      "Epoch: [133/1000], Step: [200/3], Loss: 0.0806\n",
      "Epoch: [133/1000], Step: [300/3], Loss: 0.0803\n",
      "Epoch: [134/1000], Step: [100/3], Loss: 0.0797\n",
      "Epoch: [134/1000], Step: [200/3], Loss: 0.0794\n",
      "Epoch: [134/1000], Step: [300/3], Loss: 0.0791\n",
      "Epoch: [135/1000], Step: [100/3], Loss: 0.0786\n",
      "Epoch: [135/1000], Step: [200/3], Loss: 0.0783\n",
      "Epoch: [135/1000], Step: [300/3], Loss: 0.0780\n",
      "Epoch: [136/1000], Step: [100/3], Loss: 0.0774\n",
      "Epoch: [136/1000], Step: [200/3], Loss: 0.0771\n",
      "Epoch: [136/1000], Step: [300/3], Loss: 0.0769\n",
      "Epoch: [137/1000], Step: [100/3], Loss: 0.0763\n",
      "Epoch: [137/1000], Step: [200/3], Loss: 0.0760\n",
      "Epoch: [137/1000], Step: [300/3], Loss: 0.0758\n",
      "Epoch: [138/1000], Step: [100/3], Loss: 0.0752\n",
      "Epoch: [138/1000], Step: [200/3], Loss: 0.0750\n",
      "Epoch: [138/1000], Step: [300/3], Loss: 0.0747\n",
      "Epoch: [139/1000], Step: [100/3], Loss: 0.0742\n",
      "Epoch: [139/1000], Step: [200/3], Loss: 0.0739\n",
      "Epoch: [139/1000], Step: [300/3], Loss: 0.0736\n",
      "Epoch: [140/1000], Step: [100/3], Loss: 0.0731\n",
      "Epoch: [140/1000], Step: [200/3], Loss: 0.0729\n",
      "Epoch: [140/1000], Step: [300/3], Loss: 0.0726\n",
      "Epoch: [141/1000], Step: [100/3], Loss: 0.0721\n",
      "Epoch: [141/1000], Step: [200/3], Loss: 0.0719\n",
      "Epoch: [141/1000], Step: [300/3], Loss: 0.0716\n",
      "Epoch: [142/1000], Step: [100/3], Loss: 0.0711\n",
      "Epoch: [142/1000], Step: [200/3], Loss: 0.0709\n",
      "Epoch: [142/1000], Step: [300/3], Loss: 0.0706\n",
      "Epoch: [143/1000], Step: [100/3], Loss: 0.0701\n",
      "Epoch: [143/1000], Step: [200/3], Loss: 0.0699\n",
      "Epoch: [143/1000], Step: [300/3], Loss: 0.0696\n",
      "Epoch: [144/1000], Step: [100/3], Loss: 0.0692\n",
      "Epoch: [144/1000], Step: [200/3], Loss: 0.0689\n",
      "Epoch: [144/1000], Step: [300/3], Loss: 0.0687\n",
      "Epoch: [145/1000], Step: [100/3], Loss: 0.0682\n",
      "Epoch: [145/1000], Step: [200/3], Loss: 0.0680\n",
      "Epoch: [145/1000], Step: [300/3], Loss: 0.0678\n",
      "Epoch: [146/1000], Step: [100/3], Loss: 0.0673\n",
      "Epoch: [146/1000], Step: [200/3], Loss: 0.0671\n",
      "Epoch: [146/1000], Step: [300/3], Loss: 0.0669\n",
      "Epoch: [147/1000], Step: [100/3], Loss: 0.0664\n",
      "Epoch: [147/1000], Step: [200/3], Loss: 0.0662\n",
      "Epoch: [147/1000], Step: [300/3], Loss: 0.0660\n",
      "Epoch: [148/1000], Step: [100/3], Loss: 0.0656\n",
      "Epoch: [148/1000], Step: [200/3], Loss: 0.0653\n",
      "Epoch: [148/1000], Step: [300/3], Loss: 0.0651\n",
      "Epoch: [149/1000], Step: [100/3], Loss: 0.0647\n",
      "Epoch: [149/1000], Step: [200/3], Loss: 0.0645\n",
      "Epoch: [149/1000], Step: [300/3], Loss: 0.0643\n",
      "Epoch: [150/1000], Step: [100/3], Loss: 0.0639\n",
      "Epoch: [150/1000], Step: [200/3], Loss: 0.0636\n",
      "Epoch: [150/1000], Step: [300/3], Loss: 0.0634\n",
      "Epoch: [151/1000], Step: [100/3], Loss: 0.0630\n",
      "Epoch: [151/1000], Step: [200/3], Loss: 0.0628\n",
      "Epoch: [151/1000], Step: [300/3], Loss: 0.0626\n",
      "Epoch: [152/1000], Step: [100/3], Loss: 0.0622\n",
      "Epoch: [152/1000], Step: [200/3], Loss: 0.0620\n",
      "Epoch: [152/1000], Step: [300/3], Loss: 0.0618\n",
      "Epoch: [153/1000], Step: [100/3], Loss: 0.0614\n",
      "Epoch: [153/1000], Step: [200/3], Loss: 0.0612\n",
      "Epoch: [153/1000], Step: [300/3], Loss: 0.0610\n",
      "Epoch: [154/1000], Step: [100/3], Loss: 0.0607\n",
      "Epoch: [154/1000], Step: [200/3], Loss: 0.0605\n",
      "Epoch: [154/1000], Step: [300/3], Loss: 0.0603\n",
      "Epoch: [155/1000], Step: [100/3], Loss: 0.0599\n",
      "Epoch: [155/1000], Step: [200/3], Loss: 0.0597\n",
      "Epoch: [155/1000], Step: [300/3], Loss: 0.0595\n",
      "Epoch: [156/1000], Step: [100/3], Loss: 0.0592\n",
      "Epoch: [156/1000], Step: [200/3], Loss: 0.0590\n",
      "Epoch: [156/1000], Step: [300/3], Loss: 0.0588\n",
      "Epoch: [157/1000], Step: [100/3], Loss: 0.0584\n",
      "Epoch: [157/1000], Step: [200/3], Loss: 0.0582\n",
      "Epoch: [157/1000], Step: [300/3], Loss: 0.0581\n",
      "Epoch: [158/1000], Step: [100/3], Loss: 0.0577\n",
      "Epoch: [158/1000], Step: [200/3], Loss: 0.0575\n",
      "Epoch: [158/1000], Step: [300/3], Loss: 0.0574\n",
      "Epoch: [159/1000], Step: [100/3], Loss: 0.0570\n",
      "Epoch: [159/1000], Step: [200/3], Loss: 0.0568\n",
      "Epoch: [159/1000], Step: [300/3], Loss: 0.0567\n",
      "Epoch: [160/1000], Step: [100/3], Loss: 0.0563\n",
      "Epoch: [160/1000], Step: [200/3], Loss: 0.0562\n",
      "Epoch: [160/1000], Step: [300/3], Loss: 0.0560\n",
      "Epoch: [161/1000], Step: [100/3], Loss: 0.0556\n",
      "Epoch: [161/1000], Step: [200/3], Loss: 0.0555\n",
      "Epoch: [161/1000], Step: [300/3], Loss: 0.0553\n",
      "Epoch: [162/1000], Step: [100/3], Loss: 0.0550\n",
      "Epoch: [162/1000], Step: [200/3], Loss: 0.0548\n",
      "Epoch: [162/1000], Step: [300/3], Loss: 0.0547\n",
      "Epoch: [163/1000], Step: [100/3], Loss: 0.0543\n",
      "Epoch: [163/1000], Step: [200/3], Loss: 0.0542\n",
      "Epoch: [163/1000], Step: [300/3], Loss: 0.0540\n",
      "Epoch: [164/1000], Step: [100/3], Loss: 0.0537\n",
      "Epoch: [164/1000], Step: [200/3], Loss: 0.0535\n",
      "Epoch: [164/1000], Step: [300/3], Loss: 0.0534\n",
      "Epoch: [165/1000], Step: [100/3], Loss: 0.0531\n",
      "Epoch: [165/1000], Step: [200/3], Loss: 0.0529\n",
      "Epoch: [165/1000], Step: [300/3], Loss: 0.0528\n",
      "Epoch: [166/1000], Step: [100/3], Loss: 0.0525\n",
      "Epoch: [166/1000], Step: [200/3], Loss: 0.0523\n",
      "Epoch: [166/1000], Step: [300/3], Loss: 0.0522\n",
      "Epoch: [167/1000], Step: [100/3], Loss: 0.0519\n",
      "Epoch: [167/1000], Step: [200/3], Loss: 0.0517\n",
      "Epoch: [167/1000], Step: [300/3], Loss: 0.0516\n",
      "Epoch: [168/1000], Step: [100/3], Loss: 0.0513\n",
      "Epoch: [168/1000], Step: [200/3], Loss: 0.0511\n",
      "Epoch: [168/1000], Step: [300/3], Loss: 0.0510\n",
      "Epoch: [169/1000], Step: [100/3], Loss: 0.0507\n",
      "Epoch: [169/1000], Step: [200/3], Loss: 0.0506\n",
      "Epoch: [169/1000], Step: [300/3], Loss: 0.0504\n",
      "Epoch: [170/1000], Step: [100/3], Loss: 0.0501\n",
      "Epoch: [170/1000], Step: [200/3], Loss: 0.0500\n",
      "Epoch: [170/1000], Step: [300/3], Loss: 0.0498\n",
      "Epoch: [171/1000], Step: [100/3], Loss: 0.0496\n",
      "Epoch: [171/1000], Step: [200/3], Loss: 0.0494\n",
      "Epoch: [171/1000], Step: [300/3], Loss: 0.0493\n",
      "Epoch: [172/1000], Step: [100/3], Loss: 0.0490\n",
      "Epoch: [172/1000], Step: [200/3], Loss: 0.0489\n",
      "Epoch: [172/1000], Step: [300/3], Loss: 0.0488\n",
      "Epoch: [173/1000], Step: [100/3], Loss: 0.0485\n",
      "Epoch: [173/1000], Step: [200/3], Loss: 0.0483\n",
      "Epoch: [173/1000], Step: [300/3], Loss: 0.0482\n",
      "Epoch: [174/1000], Step: [100/3], Loss: 0.0480\n",
      "Epoch: [174/1000], Step: [200/3], Loss: 0.0478\n",
      "Epoch: [174/1000], Step: [300/3], Loss: 0.0477\n",
      "Epoch: [175/1000], Step: [100/3], Loss: 0.0474\n",
      "Epoch: [175/1000], Step: [200/3], Loss: 0.0473\n",
      "Epoch: [175/1000], Step: [300/3], Loss: 0.0472\n",
      "Epoch: [176/1000], Step: [100/3], Loss: 0.0469\n",
      "Epoch: [176/1000], Step: [200/3], Loss: 0.0468\n",
      "Epoch: [176/1000], Step: [300/3], Loss: 0.0467\n",
      "Epoch: [177/1000], Step: [100/3], Loss: 0.0464\n",
      "Epoch: [177/1000], Step: [200/3], Loss: 0.0463\n",
      "Epoch: [177/1000], Step: [300/3], Loss: 0.0462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [178/1000], Step: [100/3], Loss: 0.0459\n",
      "Epoch: [178/1000], Step: [200/3], Loss: 0.0458\n",
      "Epoch: [178/1000], Step: [300/3], Loss: 0.0457\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "input_size = len(base_vector)\n",
    "num_classes = 5\n",
    "num_epochs = 1000\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (tweet_vector) in enumerate(tweet_vectors):\n",
    "        tensor_tweet_vector = Variable(torch.FloatTensor(tweet_vectors))\n",
    "        labels = Variable(torch.LongTensor(category_labels))\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tensor_tweet_vector)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' % (\n",
    "            epoch + 1, num_epochs, i + 1, len(tensor_tweet_vectors) // batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for test_tweet_vector in test_tweet_vectors:\n",
    "    tensor_test_tweet_vector = Variable(torch.FloatTensor(test_tweet_vectors))\n",
    "    outputs = model(tensor_test_tweet_vector)\n",
    "    test_labels = torch.LongTensor(test_category_labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += test_labels.size(0)\n",
    "    correct += (predicted == test_labels).sum()\n",
    "\n",
    "print('Accuracy of the model on the 100 test: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Model\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
