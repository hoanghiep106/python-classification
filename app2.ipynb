{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv_loader import tweets, test_tweets\n",
    "from db import tweets as db_tweets\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def check_base_word(base_word):\n",
    "\tif base_word and len(base_word) > 1 and base_word.isalpha():\n",
    "\t\tstemmed_word = stemmer.stem(base_word)\n",
    "\t\tif stemmed_word not in base_vector and stemmed_word not in stopwords.words('english'):\n",
    "\t\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "\n",
    "# Create base vector\n",
    "base_vector = []\n",
    "for tweet in tweets:\n",
    "\tif tweet['text']:\n",
    "\t\twords = tweet['text'].split()\n",
    "\t\tfor word in words:\n",
    "\t\t\tif check_base_word(word):\n",
    "\t\t\t\tbase_vector.append(stemmer.stem(word))\n",
    "                \n",
    "\n",
    "# Transform tweets into vectors\n",
    "tweet_vectors = []\n",
    "category_labels = []\n",
    "subcategory_labels = []\n",
    "for tweet in tweets:\n",
    "    category_labels.append(tweet['category'])\n",
    "    subcategory_labels.append(tweet['subcategory'])\n",
    "    words = list(set(tweet['text'].split()))\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word and len(word) > 1 and word.isalpha()]\n",
    "    tweet_vector = []\n",
    "    for ele in base_vector:\n",
    "        tweet_vector.append(1 if ele in stemmed_words else 0)\n",
    "    tweet_vectors.append(tweet_vector)\n",
    "        \n",
    "# Transform tweets into vectors\n",
    "test_tweet_vectors = []\n",
    "test_category_labels = []\n",
    "test_subcategory_labels = []\n",
    "for tweet in test_tweets:\n",
    "    test_category_labels.append(tweet['category'])\n",
    "    test_subcategory_labels.append(tweet['subcategory'])\n",
    "    words = list(set(tweet['text'].split()))\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word and len(word) > 1 and word.isalpha()]\n",
    "    test_tweet_vector = []\n",
    "    for ele in base_vector:\n",
    "        test_tweet_vector.append(1 if ele in stemmed_words else 0)\n",
    "    test_tweet_vectors.append(test_tweet_vector)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "input_size = 1314\n",
    "num_classes = 5\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "\tdef __init__(self, input_size, num_classes):\n",
    "\t\tsuper(LogisticRegression, self).__init__()\n",
    "\t\tself.linear = nn.Linear(input_size, 200) # Co the doi 200 thanh so khac. cung ko bit so nao dung y_1 = W_1x\n",
    "\t\tself.relu = nn.ReLU() # non-linearity. do y=sigma(W_1x)\n",
    "\t\tself.linear2 = nn.Linear(200, num_classes) # y = W_2 sigma(W_1 x). \n",
    "        #Em co the them 1 layer nua hien tai la no map tu input_size -> 200 -> num_classes \n",
    "        # Them layer nua nhu la input_size -> 200 -> 100 -> num_classes\n",
    "        # roi them non-linearity. em search cai relu\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.linear(x)\n",
    "\t\tout = self.relu(out)\n",
    "\t\tout = self.linear2(out)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Loss: 1.6060\n",
      "Epoch: [101/5], Loss: 1.5399\n",
      "Epoch: [201/5], Loss: 1.4790\n",
      "Epoch: [301/5], Loss: 1.4227\n",
      "Epoch: [401/5], Loss: 1.3708\n",
      "Epoch: [501/5], Loss: 1.3231\n",
      "Epoch: [601/5], Loss: 1.2798\n",
      "Epoch: [701/5], Loss: 1.2407\n",
      "Epoch: [801/5], Loss: 1.2058\n",
      "Epoch: [901/5], Loss: 1.1751\n",
      "Epoch: [1001/5], Loss: 1.1484\n",
      "Epoch: [1101/5], Loss: 1.1256\n",
      "Epoch: [1201/5], Loss: 1.1062\n",
      "Epoch: [1301/5], Loss: 1.0898\n",
      "Epoch: [1401/5], Loss: 1.0760\n",
      "Epoch: [1501/5], Loss: 1.0644\n",
      "Epoch: [1601/5], Loss: 1.0544\n",
      "Epoch: [1701/5], Loss: 1.0459\n",
      "Epoch: [1801/5], Loss: 1.0385\n",
      "Epoch: [1901/5], Loss: 1.0319\n",
      "Epoch: [2001/5], Loss: 1.0259\n",
      "Epoch: [2101/5], Loss: 1.0204\n",
      "Epoch: [2201/5], Loss: 1.0153\n",
      "Epoch: [2301/5], Loss: 1.0105\n",
      "Epoch: [2401/5], Loss: 1.0058\n",
      "Epoch: [2501/5], Loss: 1.0014\n",
      "Epoch: [2601/5], Loss: 0.9970\n",
      "Epoch: [2701/5], Loss: 0.9928\n",
      "Epoch: [2801/5], Loss: 0.9886\n",
      "Epoch: [2901/5], Loss: 0.9844\n",
      "Epoch: [3001/5], Loss: 0.9803\n",
      "Epoch: [3101/5], Loss: 0.9762\n",
      "Epoch: [3201/5], Loss: 0.9721\n",
      "Epoch: [3301/5], Loss: 0.9680\n",
      "Epoch: [3401/5], Loss: 0.9639\n",
      "Epoch: [3501/5], Loss: 0.9597\n",
      "Epoch: [3601/5], Loss: 0.9556\n",
      "Epoch: [3701/5], Loss: 0.9514\n",
      "Epoch: [3801/5], Loss: 0.9471\n",
      "Epoch: [3901/5], Loss: 0.9429\n",
      "Epoch: [4001/5], Loss: 0.9386\n",
      "Epoch: [4101/5], Loss: 0.9342\n",
      "Epoch: [4201/5], Loss: 0.9298\n",
      "Epoch: [4301/5], Loss: 0.9253\n",
      "Epoch: [4401/5], Loss: 0.9208\n",
      "Epoch: [4501/5], Loss: 0.9163\n",
      "Epoch: [4601/5], Loss: 0.9117\n",
      "Epoch: [4701/5], Loss: 0.9070\n",
      "Epoch: [4801/5], Loss: 0.9023\n",
      "Epoch: [4901/5], Loss: 0.8975\n",
      "Epoch: [5001/5], Loss: 0.8927\n",
      "Epoch: [5101/5], Loss: 0.8879\n",
      "Epoch: [5201/5], Loss: 0.8829\n",
      "Epoch: [5301/5], Loss: 0.8780\n",
      "Epoch: [5401/5], Loss: 0.8730\n",
      "Epoch: [5501/5], Loss: 0.8679\n",
      "Epoch: [5601/5], Loss: 0.8628\n",
      "Epoch: [5701/5], Loss: 0.8576\n",
      "Epoch: [5801/5], Loss: 0.8524\n",
      "Epoch: [5901/5], Loss: 0.8471\n",
      "Epoch: [6001/5], Loss: 0.8417\n",
      "Epoch: [6101/5], Loss: 0.8363\n",
      "Epoch: [6201/5], Loss: 0.8308\n",
      "Epoch: [6301/5], Loss: 0.8253\n",
      "Epoch: [6401/5], Loss: 0.8198\n",
      "Epoch: [6501/5], Loss: 0.8142\n",
      "Epoch: [6601/5], Loss: 0.8085\n",
      "Epoch: [6701/5], Loss: 0.8028\n",
      "Epoch: [6801/5], Loss: 0.7971\n",
      "Epoch: [6901/5], Loss: 0.7914\n",
      "Epoch: [7001/5], Loss: 0.7856\n",
      "Epoch: [7101/5], Loss: 0.7797\n",
      "Epoch: [7201/5], Loss: 0.7739\n",
      "Epoch: [7301/5], Loss: 0.7680\n",
      "Epoch: [7401/5], Loss: 0.7621\n",
      "Epoch: [7501/5], Loss: 0.7561\n",
      "Epoch: [7601/5], Loss: 0.7501\n",
      "Epoch: [7701/5], Loss: 0.7441\n",
      "Epoch: [7801/5], Loss: 0.7381\n",
      "Epoch: [7901/5], Loss: 0.7321\n",
      "Epoch: [8001/5], Loss: 0.7261\n",
      "Epoch: [8101/5], Loss: 0.7200\n",
      "Epoch: [8201/5], Loss: 0.7139\n",
      "Epoch: [8301/5], Loss: 0.7078\n",
      "Epoch: [8401/5], Loss: 0.7017\n",
      "Epoch: [8501/5], Loss: 0.6956\n",
      "Epoch: [8601/5], Loss: 0.6895\n",
      "Epoch: [8701/5], Loss: 0.6834\n",
      "Epoch: [8801/5], Loss: 0.6773\n",
      "Epoch: [8901/5], Loss: 0.6711\n",
      "Epoch: [9001/5], Loss: 0.6650\n",
      "Epoch: [9101/5], Loss: 0.6589\n",
      "Epoch: [9201/5], Loss: 0.6528\n",
      "Epoch: [9301/5], Loss: 0.6467\n",
      "Epoch: [9401/5], Loss: 0.6406\n",
      "Epoch: [9501/5], Loss: 0.6345\n",
      "Epoch: [9601/5], Loss: 0.6285\n",
      "Epoch: [9701/5], Loss: 0.6224\n",
      "Epoch: [9801/5], Loss: 0.6164\n",
      "Epoch: [9901/5], Loss: 0.6105\n",
      "Epoch: [10001/5], Loss: 0.6045\n",
      "Epoch: [10101/5], Loss: 0.5986\n",
      "Epoch: [10201/5], Loss: 0.5927\n",
      "Epoch: [10301/5], Loss: 0.5868\n",
      "Epoch: [10401/5], Loss: 0.5810\n",
      "Epoch: [10501/5], Loss: 0.5752\n",
      "Epoch: [10601/5], Loss: 0.5694\n",
      "Epoch: [10701/5], Loss: 0.5637\n",
      "Epoch: [10801/5], Loss: 0.5580\n",
      "Epoch: [10901/5], Loss: 0.5523\n",
      "Epoch: [11001/5], Loss: 0.5467\n",
      "Epoch: [11101/5], Loss: 0.5412\n",
      "Epoch: [11201/5], Loss: 0.5356\n",
      "Epoch: [11301/5], Loss: 0.5302\n",
      "Epoch: [11401/5], Loss: 0.5247\n",
      "Epoch: [11501/5], Loss: 0.5193\n",
      "Epoch: [11601/5], Loss: 0.5140\n",
      "Epoch: [11701/5], Loss: 0.5087\n",
      "Epoch: [11801/5], Loss: 0.5035\n",
      "Epoch: [11901/5], Loss: 0.4983\n",
      "Epoch: [12001/5], Loss: 0.4931\n",
      "Epoch: [12101/5], Loss: 0.4880\n",
      "Epoch: [12201/5], Loss: 0.4830\n",
      "Epoch: [12301/5], Loss: 0.4780\n",
      "Epoch: [12401/5], Loss: 0.4730\n",
      "Epoch: [12501/5], Loss: 0.4681\n",
      "Epoch: [12601/5], Loss: 0.4633\n",
      "Epoch: [12701/5], Loss: 0.4585\n",
      "Epoch: [12801/5], Loss: 0.4537\n",
      "Epoch: [12901/5], Loss: 0.4490\n",
      "Epoch: [13001/5], Loss: 0.4444\n",
      "Epoch: [13101/5], Loss: 0.4398\n",
      "Epoch: [13201/5], Loss: 0.4352\n",
      "Epoch: [13301/5], Loss: 0.4307\n",
      "Epoch: [13401/5], Loss: 0.4263\n",
      "Epoch: [13501/5], Loss: 0.4219\n",
      "Epoch: [13601/5], Loss: 0.4175\n",
      "Epoch: [13701/5], Loss: 0.4132\n",
      "Epoch: [13801/5], Loss: 0.4090\n",
      "Epoch: [13901/5], Loss: 0.4048\n",
      "Epoch: [14001/5], Loss: 0.4006\n",
      "Epoch: [14101/5], Loss: 0.3965\n",
      "Epoch: [14201/5], Loss: 0.3925\n",
      "Epoch: [14301/5], Loss: 0.3885\n",
      "Epoch: [14401/5], Loss: 0.3845\n",
      "Epoch: [14501/5], Loss: 0.3806\n",
      "Epoch: [14601/5], Loss: 0.3767\n",
      "Epoch: [14701/5], Loss: 0.3729\n",
      "Epoch: [14801/5], Loss: 0.3691\n",
      "Epoch: [14901/5], Loss: 0.3654\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "for epoch in range(15000):\n",
    "\t#for i, (tv) in enumerate(tweet_vectors):\n",
    "    tensor_tweet_vector = Variable(torch.FloatTensor(tweet_vectors))\n",
    "    labels = Variable(torch.LongTensor(category_labels))\n",
    "\n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(tensor_tweet_vector)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: [%d/%d], Loss: %.4f' % (\n",
    "            epoch + 1, num_epochs, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 100 test: 80 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "images = Variable(torch.FloatTensor(test_tweet_vectors))\n",
    "outputs = model(images)\n",
    "test_labels = torch.LongTensor(test_category_labels)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total += test_labels.size(0)\n",
    "correct += (predicted == test_labels).sum()\n",
    "\n",
    "print('Accuracy of the model on the 100 test: %d %%' % (100 * correct / total))\n",
    "\n",
    "from db import tweets as db\n",
    "\n",
    "for tweet in db:\n",
    "    print(tweets['text'])\n",
    "    words = list(set(tweet['text'].split()))\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word and len(word) > 1 and word.isalpha()]\n",
    "    tweet_vector = []\n",
    "    for ele in base_vector:\n",
    "        tweet_vector.append(1 if ele in stemmed_words else 0)\n",
    "    images = Variable(torch.FloatTensor(tweet_vector))\n",
    "    output = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(predicted)\n",
    "\n",
    "# Save the Model\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
